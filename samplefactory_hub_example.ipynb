{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "This notebook explains how to run experiments using Sample-Factory, as well as upload and download models from the Hugging Face Hub. We will use OpenAI Gym's Lunar Lander environment as an example.\n",
    "\n",
    "\n",
    "**Step 1: Install Dependencies**\n",
    "\n",
    "To run this notebook, we need to install both `sample-factory` and the Lunar Lander environment using `pip`. Additional setup is required to use the Hugging Face Hub and instructions can be found at https://alex-petrenko.github.io/sample-factory/get-started/huggingface/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sample-factory\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Create Lunar Lander Environemnt and Specify Training Parameters**\n",
    "\n",
    "First, we need to create the Lunar Lander training environment. We can do so using Sample-Factory's `make_gym_env_func` to register the environment.\n",
    "\n",
    "We also need to specify some parameters for our experiment. All experiments need to specify `algo` which is the algorithm used to train, `env` which is the environment we are running on, and `experiment` which is where to save the model after running the experiment.\n",
    "\n",
    "Other training parameters can be specified as well. A full list of parameters can be found by running Sample-Factory with the `--help` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sf_examples.train_gym_env import make_gym_env_func, parse_custom_args\n",
    "from sample_factory.envs.env_utils import register_env\n",
    "\n",
    "# Register Lunar Lander environment\n",
    "register_env(\"LunarLanderContinuous-v2\", make_gym_env_func)\n",
    "\n",
    "# Initialize basic arguments for running the experiment. These parameters are required to run any experiment\n",
    "# The parameters can also be specified in the command line\n",
    "experiment_name = \"lunar_lander_example\"\n",
    "argv = [\"--algo=APPO\", \"--env=LunarLanderContinuous-v2\", f\"--experiment={experiment_name}\"]\n",
    "cfg = parse_custom_args(argv=argv, evaluation=False)\n",
    "\n",
    "# The following parameters can be changed from the default\n",
    "cfg.reward_scale = 0.05\n",
    "cfg.train_for_env_steps = 5000000\n",
    "cfg.gae_lambda = 0.99\n",
    "cfg.num_workers = 20\n",
    "cfg.num_envs_per_worker = 6\n",
    "cfg.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Run Experiment**\n",
    "\n",
    "Next, we train the experiment using the parameters we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2022-10-26 22:36:54,887][12030] Saved parameter configuration for experiment lunar_lander_example not found!\u001b[0m\n",
      "\u001b[33m[2022-10-26 22:36:54,889][12030] Starting experiment from scratch!\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:54,894][12030] Experiment dir /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example already exists!\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:54,894][12030] Resuming existing experiment from /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:54,895][12030] Weights and Biases integration disabled\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:54,897][12030] Environment var CUDA_VISIBLE_DEVICES is 0\n",
      "\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:55,856][20411] Env info: EnvInfo(obs_space=Dict('obs': Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
      " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
      " 1.       ], (8,), float32)), action_space=Box(-1.0, 1.0, (2,), float32), num_agents=1, gpu_actions=False, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,003][12030] Starting experiment with the following configuration:\n",
      "help=False\n",
      "algo=APPO\n",
      "env=LunarLanderContinuous-v2\n",
      "experiment=lunar_lander_example\n",
      "train_dir=/home/andrew_huggingface_co/sample-factory/train_dir\n",
      "restart_behavior=resume\n",
      "device=gpu\n",
      "seed=None\n",
      "num_policies=1\n",
      "async_rl=True\n",
      "serial_mode=False\n",
      "batched_sampling=False\n",
      "num_batches_to_accumulate=2\n",
      "worker_num_splits=2\n",
      "policy_workers_per_policy=1\n",
      "max_policy_lag=1000\n",
      "num_workers=8\n",
      "num_envs_per_worker=2\n",
      "batch_size=1024\n",
      "num_batches_per_epoch=1\n",
      "num_epochs=1\n",
      "rollout=32\n",
      "recurrence=32\n",
      "shuffle_minibatches=False\n",
      "gamma=0.99\n",
      "reward_scale=0.05\n",
      "reward_clip=1000.0\n",
      "value_bootstrap=False\n",
      "normalize_returns=False\n",
      "exploration_loss_coeff=0.003\n",
      "value_loss_coeff=0.5\n",
      "kl_loss_coeff=0.0\n",
      "exploration_loss=entropy\n",
      "gae_lambda=0.95\n",
      "ppo_clip_ratio=0.1\n",
      "ppo_clip_value=1.0\n",
      "with_vtrace=False\n",
      "vtrace_rho=1.0\n",
      "vtrace_c=1.0\n",
      "optimizer=adam\n",
      "adam_eps=1e-06\n",
      "adam_beta1=0.9\n",
      "adam_beta2=0.999\n",
      "max_grad_norm=4.0\n",
      "learning_rate=0.0001\n",
      "lr_schedule=constant\n",
      "lr_schedule_kl_threshold=0.008\n",
      "obs_subtract_mean=0.0\n",
      "obs_scale=1.0\n",
      "normalize_input=False\n",
      "normalize_input_keys=None\n",
      "decorrelate_experience_max_seconds=0\n",
      "decorrelate_envs_on_one_worker=True\n",
      "actor_worker_gpus=[]\n",
      "set_workers_cpu_affinity=True\n",
      "force_envs_single_thread=False\n",
      "default_niceness=0\n",
      "log_to_file=True\n",
      "experiment_summaries_interval=10\n",
      "flush_summaries_interval=30\n",
      "stats_avg=100\n",
      "summaries_use_frameskip=True\n",
      "heartbeat_interval=10\n",
      "heartbeat_reporting_interval=60\n",
      "train_for_env_steps=5000000\n",
      "train_for_seconds=10000000000\n",
      "save_every_sec=120\n",
      "keep_checkpoints=2\n",
      "load_checkpoint_kind=latest\n",
      "save_milestones_sec=-1\n",
      "save_best_every_sec=5\n",
      "save_best_metric=reward\n",
      "save_best_after=100000\n",
      "benchmark=False\n",
      "encoder_mlp_layers=[512, 512]\n",
      "encoder_conv_architecture=convnet_simple\n",
      "encoder_conv_mlp_layers=[512]\n",
      "use_rnn=True\n",
      "rnn_size=512\n",
      "rnn_type=gru\n",
      "rnn_num_layers=1\n",
      "decoder_mlp_layers=[]\n",
      "nonlinearity=elu\n",
      "policy_initialization=orthogonal\n",
      "policy_init_gain=1.0\n",
      "actor_critic_share_weights=True\n",
      "adaptive_stddev=True\n",
      "continuous_tanh_scale=0.0\n",
      "initial_stddev=1.0\n",
      "use_env_info_cache=False\n",
      "env_gpu_actions=False\n",
      "env_gpu_observations=True\n",
      "env_frameskip=1\n",
      "env_framestack=1\n",
      "pixel_format=CHW\n",
      "use_record_episode_statistics=False\n",
      "with_wandb=False\n",
      "wandb_user=None\n",
      "wandb_project=sample_factory\n",
      "wandb_group=None\n",
      "wandb_job_type=SF\n",
      "wandb_tags=[]\n",
      "with_pbt=False\n",
      "pbt_mix_policies_in_one_env=True\n",
      "pbt_period_env_steps=5000000\n",
      "pbt_start_mutation=20000000\n",
      "pbt_replace_fraction=0.3\n",
      "pbt_mutation_rate=0.15\n",
      "pbt_replace_reward_gap=0.1\n",
      "pbt_replace_reward_gap_absolute=1e-06\n",
      "pbt_optimize_gamma=False\n",
      "pbt_target_objective=true_objective\n",
      "pbt_perturb_min=1.1\n",
      "pbt_perturb_max=1.5\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,005][12030] Saving configuration to /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/cfg.json...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,017][12030] Rollout worker 0 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,019][12030] Rollout worker 1 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,020][12030] Rollout worker 2 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,021][12030] Rollout worker 3 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,022][12030] Rollout worker 4 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,023][12030] Rollout worker 5 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,024][12030] Rollout worker 6 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,025][12030] Rollout worker 7 uses device cpu\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,052][12030] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:56,054][12030] InferenceWorker_p0-w0: min num requests: 2\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,096][12030] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,097][12030] Starting process learner_proc0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,146][12030] Starting all processes...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,153][12030] Starting process inference_proc0-0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,154][12030] Starting process rollout_proc0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc1\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc2\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc3\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc4\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc5\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc6\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:56,155][12030] Starting process rollout_proc7\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,208][20447] Rollout worker 3 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,209][20447] ROLLOUT worker 3\tpid 20447\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,229][20447] Worker 3 uses CPU cores [3]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,234][20432] LearnerWorker_p0\tpid 20432\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,235][20432] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,235][20432] Set environment var CUDA_VISIBLE_DEVICES to '0' for learning process 0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,479][20446] Rollout worker 2 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,480][20446] ROLLOUT worker 2\tpid 20446\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,481][20446] Worker 2 uses CPU cores [2]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,513][20445] InferenceWorker_p0-w0\tpid 20445\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,513][20445] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,514][20445] Set environment var CUDA_VISIBLE_DEVICES to '0' for inference process 0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,634][20449] Rollout worker 1 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,634][20449] ROLLOUT worker 1\tpid 20449\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,658][20449] Worker 1 uses CPU cores [1]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,664][20448] Rollout worker 0 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,664][20448] ROLLOUT worker 0\tpid 20448\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,669][20448] Worker 0 uses CPU cores [0]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,794][20458] Rollout worker 7 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,794][20458] ROLLOUT worker 7\tpid 20458\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,797][20458] Worker 7 uses CPU cores [7]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,853][20459] Rollout worker 5 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,854][20459] ROLLOUT worker 5\tpid 20459\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,866][20459] Worker 5 uses CPU cores [5]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,894][20460] Rollout worker 6 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,894][20460] ROLLOUT worker 6\tpid 20460\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,895][20460] Worker 6 uses CPU cores [6]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,897][20457] Rollout worker 4 starting...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:58,897][20457] ROLLOUT worker 4\tpid 20457\tparent 12030\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:58,901][20457] Worker 4 uses CPU cores [4]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:59,025][20445] Visible devices: 1\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:59,027][20432] Visible devices: 1\u001b[0m\n",
      "\u001b[33m[2022-10-26 22:36:59,052][20432] WARNING! It is generally recommended to enable Fixed KL loss (https://arxiv.org/pdf/1707.06347.pdf) for continuous action tasks to avoid potential numerical issues. I.e. set --kl_loss_coeff=0.1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:36:59,053][20432] Starting seed is not provided\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:59,053][20432] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:59,053][20432] Initializing actor-critic model on device cuda:0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:59,146][20432] Created Actor Critic model with architecture:\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:36:59,146][20432] ActorCriticSharedWeights(\n",
      "  (obs_normalizer): ObservationNormalizer()\n",
      "  (encoder): MultiInputEncoder(\n",
      "    (encoders): ModuleDict(\n",
      "      (obs): MlpEncoder(\n",
      "        (mlp_head): RecursiveScriptModule(\n",
      "          original_name=Sequential\n",
      "          (0): RecursiveScriptModule(original_name=Linear)\n",
      "          (1): RecursiveScriptModule(original_name=ELU)\n",
      "          (2): RecursiveScriptModule(original_name=Linear)\n",
      "          (3): RecursiveScriptModule(original_name=ELU)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (core): ModelCoreRNN(\n",
      "    (core): GRU(512, 512)\n",
      "  )\n",
      "  (decoder): MlpDecoder(\n",
      "    (mlp): Identity()\n",
      "  )\n",
      "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (action_parameterization): ActionParameterizationDefault(\n",
      "    (distribution_linear): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:02,416][20432] Using optimizer <class 'torch.optim.adam.Adam'>\u001b[0m\n",
      "\u001b[33m[2022-10-26 22:37:02,417][20432] No checkpoints found\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:02,418][20432] Did not load from checkpoint, starting from scratch!\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:02,418][20432] Initialized policy 0 weights for model version 0\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:02,433][20432] LearnerWorker_p0 finished initialization!\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:02,433][20432] Using GPUs [0] for process 0 (actually maps to GPUs [0])\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:04,897][12030] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,039][12030] Heartbeat connected on Batcher_0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,045][12030] Heartbeat connected on LearnerWorker_p0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,061][12030] Heartbeat connected on RolloutWorker_w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,069][12030] Heartbeat connected on RolloutWorker_w1\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,075][12030] Heartbeat connected on RolloutWorker_w3\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,080][12030] Heartbeat connected on RolloutWorker_w4\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,081][12030] Heartbeat connected on RolloutWorker_w2\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,088][12030] Heartbeat connected on RolloutWorker_w5\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,095][12030] Heartbeat connected on RolloutWorker_w7\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,097][12030] Heartbeat connected on RolloutWorker_w6\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,754][12030] Inference worker 0-0 is ready!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,755][12030] All inference workers are ready! Signal rollout workers to start!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,756][12030] Heartbeat connected on InferenceWorker_p0-w0\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,802][20459] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,802][20449] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,802][20460] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,803][20448] Decorrelating experience for 0 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,803][20457] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,803][20447] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,804][20458] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,807][20446] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,807][20448] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,820][20459] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,831][20446] Decorrelating experience for 96 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,837][20460] Decorrelating experience for 128 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,858][20458] Decorrelating experience for 64 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,871][20457] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,872][20447] Decorrelating experience for 160 frames...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:06,877][20449] Decorrelating experience for 32 frames...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:08,838][20432] Signal inference workers to stop experience collection...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:08,842][20445] InferenceWorker_p0-w0: stopping experience collection\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:09,136][20432] Signal inference workers to resume experience collection...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:09,137][20445] InferenceWorker_p0-w0: resuming experience collection\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:09,897][12030] Fps is (10 sec: 1024.0, 60 sec: 1024.0, 300 sec: 1024.0). Total num frames: 5120. Throughput: 0: 1087.0. Samples: 5435. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:09,899][12030] Avg episode reward: [(0, '-228.455')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:11,274][20445] Updated weights for policy 0, policy_version 10 (0.0007)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:14,243][20445] Updated weights for policy 0, policy_version 20 (0.0005)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:14,897][12030] Fps is (10 sec: 2252.8, 60 sec: 2252.8, 300 sec: 2252.8). Total num frames: 22528. Throughput: 0: 1565.8. Samples: 15658. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:14,898][12030] Avg episode reward: [(0, '-275.860')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:17,284][20445] Updated weights for policy 0, policy_version 30 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:19,897][12030] Fps is (10 sec: 3379.2, 60 sec: 2594.1, 300 sec: 2594.1). Total num frames: 38912. Throughput: 0: 2404.6. Samples: 36069. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:19,899][12030] Avg episode reward: [(0, '-458.503')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:20,254][20445] Updated weights for policy 0, policy_version 40 (0.0005)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:23,254][20445] Updated weights for policy 0, policy_version 50 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:24,897][12030] Fps is (10 sec: 3379.2, 60 sec: 2816.0, 300 sec: 2816.0). Total num frames: 56320. Throughput: 0: 2829.9. Samples: 56597. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:24,898][12030] Avg episode reward: [(0, '-474.909')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:26,270][20445] Updated weights for policy 0, policy_version 60 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:29,304][20445] Updated weights for policy 0, policy_version 70 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:29,897][12030] Fps is (10 sec: 3481.6, 60 sec: 2949.1, 300 sec: 2949.1). Total num frames: 73728. Throughput: 0: 2670.5. Samples: 66762. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:29,898][12030] Avg episode reward: [(0, '-261.176')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:32,330][20445] Updated weights for policy 0, policy_version 80 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:34,897][12030] Fps is (10 sec: 3379.2, 60 sec: 3003.7, 300 sec: 3003.7). Total num frames: 90112. Throughput: 0: 2900.5. Samples: 87016. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:34,899][12030] Avg episode reward: [(0, '-283.990')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:35,411][20445] Updated weights for policy 0, policy_version 90 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:38,436][20445] Updated weights for policy 0, policy_version 100 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:39,897][12030] Fps is (10 sec: 3276.8, 60 sec: 3042.7, 300 sec: 3042.7). Total num frames: 106496. Throughput: 0: 3047.7. Samples: 106668. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:39,899][12030] Avg episode reward: [(0, '-153.365')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:39,900][20432] Saving new best policy, reward=-153.365!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:41,720][20445] Updated weights for policy 0, policy_version 110 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:44,897][12030] Fps is (10 sec: 3174.4, 60 sec: 3046.4, 300 sec: 3046.4). Total num frames: 121856. Throughput: 0: 2907.1. Samples: 116284. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:44,899][12030] Avg episode reward: [(0, '-120.976')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:44,928][20432] Saving new best policy, reward=-120.976!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:44,929][20445] Updated weights for policy 0, policy_version 120 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:48,148][20445] Updated weights for policy 0, policy_version 130 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:49,897][12030] Fps is (10 sec: 3174.4, 60 sec: 3072.0, 300 sec: 3072.0). Total num frames: 138240. Throughput: 0: 3011.6. Samples: 135522. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:49,899][12030] Avg episode reward: [(0, '-159.774')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:51,426][20445] Updated weights for policy 0, policy_version 140 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:54,897][12030] Fps is (10 sec: 2969.6, 60 sec: 3031.0, 300 sec: 3031.0). Total num frames: 151552. Throughput: 0: 3254.2. Samples: 151875. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:54,899][12030] Avg episode reward: [(0, '-200.491')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:55,554][20445] Updated weights for policy 0, policy_version 150 (0.0010)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:37:59,623][20445] Updated weights for policy 0, policy_version 160 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:59,897][12030] Fps is (10 sec: 2560.0, 60 sec: 2978.9, 300 sec: 2978.9). Total num frames: 163840. Throughput: 0: 3193.7. Samples: 159374. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:37:59,899][12030] Avg episode reward: [(0, '-204.676')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:04,897][12030] Fps is (10 sec: 2048.0, 60 sec: 2867.2, 300 sec: 2867.2). Total num frames: 172032. Throughput: 0: 3011.9. Samples: 171603. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:04,899][12030] Avg episode reward: [(0, '-199.627')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:05,896][20445] Updated weights for policy 0, policy_version 170 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:09,897][12030] Fps is (10 sec: 1843.2, 60 sec: 2952.5, 300 sec: 2804.2). Total num frames: 182272. Throughput: 0: 2798.5. Samples: 182528. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:09,899][12030] Avg episode reward: [(0, '-183.423')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:10,951][20445] Updated weights for policy 0, policy_version 180 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:14,897][12030] Fps is (10 sec: 2048.0, 60 sec: 2833.1, 300 sec: 2750.2). Total num frames: 192512. Throughput: 0: 2711.0. Samples: 188758. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:14,899][12030] Avg episode reward: [(0, '-160.184')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:15,513][20445] Updated weights for policy 0, policy_version 190 (0.0006)\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:19,798][20445] Updated weights for policy 0, policy_version 200 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:19,897][12030] Fps is (10 sec: 2252.8, 60 sec: 2764.8, 300 sec: 2730.7). Total num frames: 204800. Throughput: 0: 2561.5. Samples: 202284. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:19,899][12030] Avg episode reward: [(0, '-151.320')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:24,889][20445] Updated weights for policy 0, policy_version 210 (0.0008)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:24,897][12030] Fps is (10 sec: 2252.8, 60 sec: 2645.3, 300 sec: 2688.0). Total num frames: 215040. Throughput: 0: 2407.6. Samples: 215009. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:24,899][12030] Avg episode reward: [(0, '-133.103')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:29,751][20445] Updated weights for policy 0, policy_version 220 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:29,897][12030] Fps is (10 sec: 2048.0, 60 sec: 2525.9, 300 sec: 2650.4). Total num frames: 225280. Throughput: 0: 2334.4. Samples: 221333. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:29,899][12030] Avg episode reward: [(0, '-125.408')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:34,897][12030] Fps is (10 sec: 1945.6, 60 sec: 2406.4, 300 sec: 2605.5). Total num frames: 234496. Throughput: 0: 2167.4. Samples: 233056. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:34,899][12030] Avg episode reward: [(0, '-105.503')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:34,906][20432] Saving new best policy, reward=-105.503!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:35,125][20445] Updated weights for policy 0, policy_version 230 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:39,897][12030] Fps is (10 sec: 1945.6, 60 sec: 2304.0, 300 sec: 2576.2). Total num frames: 244736. Throughput: 0: 2071.8. Samples: 245107. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:39,899][12030] Avg episode reward: [(0, '-91.014')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:39,900][20432] Saving new best policy, reward=-91.014!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:40,235][20445] Updated weights for policy 0, policy_version 240 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:44,897][12030] Fps is (10 sec: 1945.6, 60 sec: 2201.6, 300 sec: 2539.5). Total num frames: 253952. Throughput: 0: 2033.8. Samples: 250893. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:44,899][12030] Avg episode reward: [(0, '-85.567')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:44,905][20432] Saving new best policy, reward=-85.567!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:45,578][20445] Updated weights for policy 0, policy_version 250 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:49,897][12030] Fps is (10 sec: 1843.2, 60 sec: 2082.1, 300 sec: 2506.4). Total num frames: 263168. Throughput: 0: 2009.2. Samples: 262019. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:49,899][12030] Avg episode reward: [(0, '-83.501')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:49,900][20432] Saving new best policy, reward=-83.501!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:51,434][20445] Updated weights for policy 0, policy_version 260 (0.0005)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:54,897][12030] Fps is (10 sec: 1843.2, 60 sec: 2013.9, 300 sec: 2476.2). Total num frames: 272384. Throughput: 0: 2009.0. Samples: 272931. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:54,899][12030] Avg episode reward: [(0, '-87.517')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:54,904][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000266_272384.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:56,883][20445] Updated weights for policy 0, policy_version 270 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:59,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1962.7, 300 sec: 2448.7). Total num frames: 281600. Throughput: 0: 1985.2. Samples: 278094. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:38:59,899][12030] Avg episode reward: [(0, '-82.282')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:38:59,900][20432] Saving new best policy, reward=-82.282!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:03,975][20445] Updated weights for policy 0, policy_version 280 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:04,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1911.5, 300 sec: 2389.3). Total num frames: 286720. Throughput: 0: 1877.4. Samples: 286769. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:04,899][12030] Avg episode reward: [(0, '-70.830')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:04,910][20432] Saving new best policy, reward=-70.830!\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1877.3, 300 sec: 2359.3). Total num frames: 294912. Throughput: 0: 1790.1. Samples: 295564. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:09,899][12030] Avg episode reward: [(0, '-67.352')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:09,901][20432] Saving new best policy, reward=-67.352!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:10,767][20445] Updated weights for policy 0, policy_version 290 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:14,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1843.2, 300 sec: 2331.6). Total num frames: 303104. Throughput: 0: 1760.6. Samples: 300562. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:14,899][12030] Avg episode reward: [(0, '-56.047')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:14,904][20432] Saving new best policy, reward=-56.047!\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:17,230][20445] Updated weights for policy 0, policy_version 300 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:19,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1757.9, 300 sec: 2298.3). Total num frames: 310272. Throughput: 0: 1701.3. Samples: 309614. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:19,899][12030] Avg episode reward: [(0, '-58.893')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:24,263][20445] Updated weights for policy 0, policy_version 310 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:24,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1706.7, 300 sec: 2267.4). Total num frames: 317440. Throughput: 0: 1631.7. Samples: 318535. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:24,899][12030] Avg episode reward: [(0, '-64.500')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:29,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1689.6, 300 sec: 2252.8). Total num frames: 326656. Throughput: 0: 1620.9. Samples: 323833. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:29,899][12030] Avg episode reward: [(0, '-61.141')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:30,483][20445] Updated weights for policy 0, policy_version 320 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:34,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1655.5, 300 sec: 2225.5). Total num frames: 333824. Throughput: 0: 1574.8. Samples: 332885. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:34,899][12030] Avg episode reward: [(0, '-69.406')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:37,409][20445] Updated weights for policy 0, policy_version 330 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:39,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1604.3, 300 sec: 2199.9). Total num frames: 340992. Throughput: 0: 1526.5. Samples: 341625. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:39,899][12030] Avg episode reward: [(0, '-72.375')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:44,478][20445] Updated weights for policy 0, policy_version 340 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:44,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1570.1, 300 sec: 2176.0). Total num frames: 348160. Throughput: 0: 1505.1. Samples: 345824. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:44,899][12030] Avg episode reward: [(0, '-84.360')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:49,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1536.0, 300 sec: 2153.5). Total num frames: 355328. Throughput: 0: 1502.0. Samples: 354357. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:49,899][12030] Avg episode reward: [(0, '-81.764')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:52,032][20445] Updated weights for policy 0, policy_version 350 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:54,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1501.9, 300 sec: 2132.3). Total num frames: 362496. Throughput: 0: 1497.0. Samples: 362927. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:54,898][12030] Avg episode reward: [(0, '-86.766')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:39:58,912][20445] Updated weights for policy 0, policy_version 360 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:59,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1467.7, 300 sec: 2112.4). Total num frames: 369664. Throughput: 0: 1484.4. Samples: 367360. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:39:59,899][12030] Avg episode reward: [(0, '-88.623')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:04,897][12030] Fps is (10 sec: 1228.8, 60 sec: 1467.7, 300 sec: 2082.1). Total num frames: 374784. Throughput: 0: 1453.7. Samples: 375032. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:04,899][12030] Avg episode reward: [(0, '-90.600')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:07,552][20445] Updated weights for policy 0, policy_version 370 (0.0011)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:09,897][12030] Fps is (10 sec: 1228.8, 60 sec: 1450.7, 300 sec: 2064.6). Total num frames: 381952. Throughput: 0: 1431.6. Samples: 382956. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:09,900][12030] Avg episode reward: [(0, '-92.174')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:13,747][20445] Updated weights for policy 0, policy_version 380 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:14,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1450.7, 300 sec: 2053.4). Total num frames: 390144. Throughput: 0: 1420.6. Samples: 387760. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:14,899][12030] Avg episode reward: [(0, '-93.436')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:19,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1450.7, 300 sec: 2037.5). Total num frames: 397312. Throughput: 0: 1417.6. Samples: 396677. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:19,898][12030] Avg episode reward: [(0, '-97.361')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:20,540][20445] Updated weights for policy 0, policy_version 390 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:24,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1484.8, 300 sec: 2032.6). Total num frames: 406528. Throughput: 0: 1453.0. Samples: 407012. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:24,898][12030] Avg episode reward: [(0, '-98.714')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:26,704][20445] Updated weights for policy 0, policy_version 400 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:29,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1450.7, 300 sec: 2018.0). Total num frames: 413696. Throughput: 0: 1460.5. Samples: 411548. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:29,899][12030] Avg episode reward: [(0, '-97.357')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:33,627][20445] Updated weights for policy 0, policy_version 410 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:34,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1450.7, 300 sec: 2004.1). Total num frames: 420864. Throughput: 0: 1463.2. Samples: 420199. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:34,899][12030] Avg episode reward: [(0, '-95.692')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:39,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1450.7, 300 sec: 1990.8). Total num frames: 428032. Throughput: 0: 1452.4. Samples: 428285. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:39,899][12030] Avg episode reward: [(0, '-96.235')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:41,165][20445] Updated weights for policy 0, policy_version 420 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:44,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1450.7, 300 sec: 1978.2). Total num frames: 435200. Throughput: 0: 1457.0. Samples: 432923. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:44,899][12030] Avg episode reward: [(0, '-96.496')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:47,938][20445] Updated weights for policy 0, policy_version 430 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:49,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1450.7, 300 sec: 1966.1). Total num frames: 442368. Throughput: 0: 1483.7. Samples: 441800. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:49,899][12030] Avg episode reward: [(0, '-98.764')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:54,005][20445] Updated weights for policy 0, policy_version 440 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:54,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1484.8, 300 sec: 1963.4). Total num frames: 451584. Throughput: 0: 1539.8. Samples: 452246. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:54,899][12030] Avg episode reward: [(0, '-102.764')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:40:54,905][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000441_451584.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:59,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1484.8, 300 sec: 1952.1). Total num frames: 458752. Throughput: 0: 1535.6. Samples: 456861. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:40:59,899][12030] Avg episode reward: [(0, '-102.469')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:00,564][20445] Updated weights for policy 0, policy_version 450 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:04,905][12030] Fps is (10 sec: 1227.9, 60 sec: 1484.6, 300 sec: 1932.7). Total num frames: 463872. Throughput: 0: 1503.8. Samples: 464358. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:04,910][12030] Avg episode reward: [(0, '-100.200')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:09,897][12030] Fps is (10 sec: 1126.4, 60 sec: 1467.7, 300 sec: 1918.4). Total num frames: 470016. Throughput: 0: 1414.6. Samples: 470669. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:09,899][12030] Avg episode reward: [(0, '-98.538')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:10,325][20445] Updated weights for policy 0, policy_version 460 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:14,897][12030] Fps is (10 sec: 1332.2, 60 sec: 1450.7, 300 sec: 1908.7). Total num frames: 477184. Throughput: 0: 1398.4. Samples: 474478. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:14,899][12030] Avg episode reward: [(0, '-98.020')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:17,634][20445] Updated weights for policy 0, policy_version 470 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:19,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1450.7, 300 sec: 1899.4). Total num frames: 484352. Throughput: 0: 1405.9. Samples: 483463. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:19,899][12030] Avg episode reward: [(0, '-97.687')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:23,404][20445] Updated weights for policy 0, policy_version 480 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:24,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1450.7, 300 sec: 1898.3). Total num frames: 493568. Throughput: 0: 1461.7. Samples: 494060. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:24,899][12030] Avg episode reward: [(0, '-96.929')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:29,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1450.7, 300 sec: 1889.6). Total num frames: 500736. Throughput: 0: 1453.6. Samples: 498336. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:29,898][12030] Avg episode reward: [(0, '-97.054')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:30,274][20445] Updated weights for policy 0, policy_version 490 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:34,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1467.7, 300 sec: 1884.9). Total num frames: 508928. Throughput: 0: 1453.8. Samples: 507220. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:34,899][12030] Avg episode reward: [(0, '-97.658')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:36,706][20445] Updated weights for policy 0, policy_version 500 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:39,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1467.7, 300 sec: 1876.7). Total num frames: 516096. Throughput: 0: 1438.6. Samples: 516985. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:39,899][12030] Avg episode reward: [(0, '-98.088')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:43,184][20445] Updated weights for policy 0, policy_version 510 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:44,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1484.8, 300 sec: 1872.5). Total num frames: 524288. Throughput: 0: 1441.5. Samples: 521730. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:44,899][12030] Avg episode reward: [(0, '-97.665')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:49,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1484.8, 300 sec: 1864.8). Total num frames: 531456. Throughput: 0: 1475.8. Samples: 530759. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:49,899][12030] Avg episode reward: [(0, '-97.586')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:50,217][20445] Updated weights for policy 0, policy_version 520 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:54,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1467.7, 300 sec: 1860.9). Total num frames: 539648. Throughput: 0: 1544.7. Samples: 540180. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:54,899][12030] Avg episode reward: [(0, '-92.547')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:41:56,762][20445] Updated weights for policy 0, policy_version 530 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:59,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1467.7, 300 sec: 1853.6). Total num frames: 546816. Throughput: 0: 1554.0. Samples: 544407. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:41:59,899][12030] Avg episode reward: [(0, '-90.126')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:04,905][12030] Fps is (10 sec: 1227.9, 60 sec: 1467.7, 300 sec: 1853.6). Total num frames: 551936. Throughput: 0: 1519.4. Samples: 551848. Policy #0 lag: (min: 0.0, avg: 0.2, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:04,906][12030] Avg episode reward: [(0, '-87.452')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:05,802][20445] Updated weights for policy 0, policy_version 540 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:09,897][12030] Fps is (10 sec: 1126.4, 60 sec: 1467.7, 300 sec: 1815.4). Total num frames: 558080. Throughput: 0: 1440.8. Samples: 558894. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:09,899][12030] Avg episode reward: [(0, '-87.951')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:13,439][20445] Updated weights for policy 0, policy_version 550 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:14,897][12030] Fps is (10 sec: 1229.7, 60 sec: 1450.7, 300 sec: 1780.7). Total num frames: 564224. Throughput: 0: 1426.6. Samples: 562531. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:14,899][12030] Avg episode reward: [(0, '-90.678')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:19,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1450.7, 300 sec: 1746.0). Total num frames: 571392. Throughput: 0: 1409.8. Samples: 570662. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:19,900][12030] Avg episode reward: [(0, '-92.344')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:20,790][20445] Updated weights for policy 0, policy_version 560 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:24,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1433.6, 300 sec: 1714.8). Total num frames: 579584. Throughput: 0: 1403.3. Samples: 580132. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:24,899][12030] Avg episode reward: [(0, '-93.209')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:27,054][20445] Updated weights for policy 0, policy_version 570 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:29,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1450.7, 300 sec: 1687.0). Total num frames: 587776. Throughput: 0: 1406.6. Samples: 585026. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:29,899][12030] Avg episode reward: [(0, '-91.566')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:33,396][20445] Updated weights for policy 0, policy_version 580 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:34,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1450.7, 300 sec: 1659.2). Total num frames: 595968. Throughput: 0: 1423.0. Samples: 594793. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:34,899][12030] Avg episode reward: [(0, '-90.118')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:39,767][20445] Updated weights for policy 0, policy_version 590 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:39,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1467.7, 300 sec: 1634.9). Total num frames: 604160. Throughput: 0: 1426.6. Samples: 604376. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:39,899][12030] Avg episode reward: [(0, '-88.350')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:44,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1467.7, 300 sec: 1607.2). Total num frames: 612352. Throughput: 0: 1458.3. Samples: 610030. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:44,899][12030] Avg episode reward: [(0, '-89.246')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:45,548][20445] Updated weights for policy 0, policy_version 600 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:49,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1484.8, 300 sec: 1589.8). Total num frames: 620544. Throughput: 0: 1497.5. Samples: 619224. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:49,900][12030] Avg episode reward: [(0, '-90.873')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:52,349][20445] Updated weights for policy 0, policy_version 610 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:54,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1467.7, 300 sec: 1572.4). Total num frames: 627712. Throughput: 0: 1546.4. Samples: 628483. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:54,899][12030] Avg episode reward: [(0, '-91.142')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:54,906][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000613_627712.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:54,957][20432] Removing /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000266_272384.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:42:59,001][20445] Updated weights for policy 0, policy_version 620 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:59,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1484.8, 300 sec: 1572.4). Total num frames: 635904. Throughput: 0: 1573.2. Samples: 633326. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:42:59,899][12030] Avg episode reward: [(0, '-92.779')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:04,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1502.1, 300 sec: 1558.6). Total num frames: 642048. Throughput: 0: 1583.1. Samples: 641903. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:04,899][12030] Avg episode reward: [(0, '-92.906')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:07,247][20445] Updated weights for policy 0, policy_version 630 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1518.9, 300 sec: 1548.1). Total num frames: 649216. Throughput: 0: 1538.9. Samples: 649381. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:09,899][12030] Avg episode reward: [(0, '-95.696')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:13,702][20445] Updated weights for policy 0, policy_version 640 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:14,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1553.1, 300 sec: 1534.3). Total num frames: 657408. Throughput: 0: 1536.6. Samples: 654172. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:14,898][12030] Avg episode reward: [(0, '-99.006')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:19,402][20445] Updated weights for policy 0, policy_version 650 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:19,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1570.1, 300 sec: 1527.3). Total num frames: 665600. Throughput: 0: 1556.3. Samples: 664826. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:19,899][12030] Avg episode reward: [(0, '-109.470')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:24,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1587.2, 300 sec: 1523.9). Total num frames: 674816. Throughput: 0: 1578.1. Samples: 675389. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:24,899][12030] Avg episode reward: [(0, '-115.254')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:25,231][20445] Updated weights for policy 0, policy_version 660 (0.0009)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:29,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1587.2, 300 sec: 1520.4). Total num frames: 683008. Throughput: 0: 1555.7. Samples: 680038. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:29,899][12030] Avg episode reward: [(0, '-119.405')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:31,125][20445] Updated weights for policy 0, policy_version 670 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:34,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1604.3, 300 sec: 1516.9). Total num frames: 692224. Throughput: 0: 1600.7. Samples: 691255. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:34,899][12030] Avg episode reward: [(0, '-121.744')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:37,028][20445] Updated weights for policy 0, policy_version 680 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:39,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1621.3, 300 sec: 1516.9). Total num frames: 701440. Throughput: 0: 1627.2. Samples: 701709. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:39,899][12030] Avg episode reward: [(0, '-122.168')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:42,549][20445] Updated weights for policy 0, policy_version 690 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:44,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1621.3, 300 sec: 1513.4). Total num frames: 709632. Throughput: 0: 1641.6. Samples: 707198. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:44,899][12030] Avg episode reward: [(0, '-123.388')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:48,389][20445] Updated weights for policy 0, policy_version 700 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:49,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1638.4, 300 sec: 1513.4). Total num frames: 718848. Throughput: 0: 1684.2. Samples: 717692. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:49,898][12030] Avg episode reward: [(0, '-125.933')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:54,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1638.4, 300 sec: 1506.5). Total num frames: 726016. Throughput: 0: 1716.4. Samples: 726619. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:54,899][12030] Avg episode reward: [(0, '-127.710')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:43:55,248][20445] Updated weights for policy 0, policy_version 710 (0.0009)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:59,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1638.4, 300 sec: 1516.9). Total num frames: 734208. Throughput: 0: 1713.1. Samples: 731262. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:43:59,899][12030] Avg episode reward: [(0, '-128.133')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:01,361][20445] Updated weights for policy 0, policy_version 720 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:04,899][12030] Fps is (10 sec: 1433.4, 60 sec: 1638.4, 300 sec: 1510.0). Total num frames: 740352. Throughput: 0: 1671.4. Samples: 740041. Policy #0 lag: (min: 0.0, avg: 0.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:04,900][12030] Avg episode reward: [(0, '-127.214')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:09,476][20445] Updated weights for policy 0, policy_version 730 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1638.4, 300 sec: 1506.5). Total num frames: 747520. Throughput: 0: 1619.4. Samples: 748262. Policy #0 lag: (min: 0.0, avg: 0.3, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:09,899][12030] Avg episode reward: [(0, '-121.056')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:14,897][12030] Fps is (10 sec: 1638.6, 60 sec: 1655.5, 300 sec: 1513.4). Total num frames: 756736. Throughput: 0: 1634.5. Samples: 753589. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:14,899][12030] Avg episode reward: [(0, '-117.159')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:15,417][20445] Updated weights for policy 0, policy_version 740 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:19,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1638.4, 300 sec: 1513.4). Total num frames: 763904. Throughput: 0: 1601.8. Samples: 763336. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:19,898][12030] Avg episode reward: [(0, '-115.412')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:21,781][20445] Updated weights for policy 0, policy_version 750 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:24,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1638.4, 300 sec: 1513.4). Total num frames: 773120. Throughput: 0: 1593.8. Samples: 773431. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:24,899][12030] Avg episode reward: [(0, '-119.963')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:27,382][20445] Updated weights for policy 0, policy_version 760 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:29,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1655.5, 300 sec: 1520.4). Total num frames: 782336. Throughput: 0: 1599.2. Samples: 779164. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:29,899][12030] Avg episode reward: [(0, '-121.092')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:33,193][20445] Updated weights for policy 0, policy_version 770 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:34,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1655.5, 300 sec: 1527.3). Total num frames: 791552. Throughput: 0: 1602.1. Samples: 789788. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:34,899][12030] Avg episode reward: [(0, '-124.295')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:38,293][20445] Updated weights for policy 0, policy_version 780 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:39,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1672.5, 300 sec: 1537.7). Total num frames: 801792. Throughput: 0: 1679.1. Samples: 802180. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:39,899][12030] Avg episode reward: [(0, '-123.588')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:43,799][20445] Updated weights for policy 0, policy_version 790 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:44,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1541.2). Total num frames: 809984. Throughput: 0: 1691.3. Samples: 807370. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:44,899][12030] Avg episode reward: [(0, '-125.463')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:48,802][20445] Updated weights for policy 0, policy_version 800 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:49,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1706.7, 300 sec: 1555.1). Total num frames: 821248. Throughput: 0: 1764.8. Samples: 819454. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:49,898][12030] Avg episode reward: [(0, '-126.864')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:54,130][20445] Updated weights for policy 0, policy_version 810 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:54,897][12030] Fps is (10 sec: 2048.0, 60 sec: 1740.8, 300 sec: 1562.0). Total num frames: 830464. Throughput: 0: 1837.5. Samples: 830950. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:54,899][12030] Avg episode reward: [(0, '-130.851')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:54,905][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000811_830464.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:54,950][20432] Removing /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000441_451584.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:44:59,600][20445] Updated weights for policy 0, policy_version 820 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:59,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1757.9, 300 sec: 1575.9). Total num frames: 839680. Throughput: 0: 1845.2. Samples: 836623. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:44:59,899][12030] Avg episode reward: [(0, '-131.614')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:04,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1775.0, 300 sec: 1575.9). Total num frames: 846848. Throughput: 0: 1850.9. Samples: 846627. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:04,899][12030] Avg episode reward: [(0, '-132.750')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:06,810][20445] Updated weights for policy 0, policy_version 830 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:09,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1809.1, 300 sec: 1579.4). Total num frames: 856064. Throughput: 0: 1840.2. Samples: 856238. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:09,899][12030] Avg episode reward: [(0, '-132.051')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:11,618][20445] Updated weights for policy 0, policy_version 840 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:14,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1826.1, 300 sec: 1589.8). Total num frames: 866304. Throughput: 0: 1859.8. Samples: 862854. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:14,900][12030] Avg episode reward: [(0, '-126.010')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:16,608][20445] Updated weights for policy 0, policy_version 850 (0.0008)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:19,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1860.3, 300 sec: 1589.8). Total num frames: 875520. Throughput: 0: 1876.1. Samples: 874214. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:19,898][12030] Avg episode reward: [(0, '-121.460')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:22,056][20445] Updated weights for policy 0, policy_version 860 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:24,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1877.3, 300 sec: 1600.2). Total num frames: 885760. Throughput: 0: 1861.4. Samples: 885943. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:24,899][12030] Avg episode reward: [(0, '-112.418')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:27,600][20445] Updated weights for policy 0, policy_version 870 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:29,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1877.3, 300 sec: 1607.2). Total num frames: 894976. Throughput: 0: 1866.7. Samples: 891370. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:29,899][12030] Avg episode reward: [(0, '-103.373')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:32,944][20445] Updated weights for policy 0, policy_version 880 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:34,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1877.3, 300 sec: 1614.1). Total num frames: 904192. Throughput: 0: 1853.1. Samples: 902844. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:34,899][12030] Avg episode reward: [(0, '-101.069')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:39,437][20445] Updated weights for policy 0, policy_version 890 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:39,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1826.1, 300 sec: 1614.1). Total num frames: 911360. Throughput: 0: 1807.6. Samples: 912292. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:39,899][12030] Avg episode reward: [(0, '-97.904')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:44,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1843.2, 300 sec: 1621.0). Total num frames: 920576. Throughput: 0: 1811.4. Samples: 918138. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:44,899][12030] Avg episode reward: [(0, '-95.881')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:44,939][20445] Updated weights for policy 0, policy_version 900 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:49,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1792.0, 300 sec: 1617.6). Total num frames: 928768. Throughput: 0: 1804.8. Samples: 927842. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:49,899][12030] Avg episode reward: [(0, '-95.079')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:51,401][20445] Updated weights for policy 0, policy_version 910 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:54,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1774.9, 300 sec: 1621.0). Total num frames: 936960. Throughput: 0: 1811.8. Samples: 937768. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:54,899][12030] Avg episode reward: [(0, '-93.896')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:45:57,444][20445] Updated weights for policy 0, policy_version 920 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:59,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1774.9, 300 sec: 1635.0). Total num frames: 946176. Throughput: 0: 1781.2. Samples: 943006. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:45:59,899][12030] Avg episode reward: [(0, '-94.567')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:04,841][20445] Updated weights for policy 0, policy_version 930 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:04,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1757.9, 300 sec: 1634.9). Total num frames: 952320. Throughput: 0: 1724.1. Samples: 951798. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:04,899][12030] Avg episode reward: [(0, '-94.803')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1723.7, 300 sec: 1634.9). Total num frames: 959488. Throughput: 0: 1643.7. Samples: 959911. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:09,898][12030] Avg episode reward: [(0, '-92.585')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:11,603][20445] Updated weights for policy 0, policy_version 940 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:14,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1689.6, 300 sec: 1638.4). Total num frames: 967680. Throughput: 0: 1634.6. Samples: 964929. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:14,899][12030] Avg episode reward: [(0, '-101.020')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:17,669][20445] Updated weights for policy 0, policy_version 950 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:19,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1672.5, 300 sec: 1634.9). Total num frames: 975872. Throughput: 0: 1604.7. Samples: 975055. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:19,899][12030] Avg episode reward: [(0, '-107.191')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:23,478][20445] Updated weights for policy 0, policy_version 960 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:24,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1655.5, 300 sec: 1641.9). Total num frames: 985088. Throughput: 0: 1625.9. Samples: 985457. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:24,899][12030] Avg episode reward: [(0, '-106.262')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:29,646][20445] Updated weights for policy 0, policy_version 970 (0.0005)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:29,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1638.4, 300 sec: 1641.9). Total num frames: 993280. Throughput: 0: 1595.7. Samples: 989946. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:29,899][12030] Avg episode reward: [(0, '-105.167')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:34,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1621.3, 300 sec: 1645.3). Total num frames: 1001472. Throughput: 0: 1616.5. Samples: 1000586. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:34,899][12030] Avg episode reward: [(0, '-104.213')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:35,644][20445] Updated weights for policy 0, policy_version 980 (0.0012)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:39,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1638.4, 300 sec: 1645.3). Total num frames: 1009664. Throughput: 0: 1617.8. Samples: 1010571. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:39,899][12030] Avg episode reward: [(0, '-105.943')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:41,963][20445] Updated weights for policy 0, policy_version 990 (0.0009)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:44,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1638.4, 300 sec: 1652.3). Total num frames: 1018880. Throughput: 0: 1608.4. Samples: 1015386. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:44,899][12030] Avg episode reward: [(0, '-110.543')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:47,441][20445] Updated weights for policy 0, policy_version 1000 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:49,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1655.5, 300 sec: 1655.8). Total num frames: 1028096. Throughput: 0: 1665.0. Samples: 1026724. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:49,899][12030] Avg episode reward: [(0, '-120.240')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:52,930][20445] Updated weights for policy 0, policy_version 1010 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:54,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1662.7). Total num frames: 1037312. Throughput: 0: 1739.6. Samples: 1038194. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:54,899][12030] Avg episode reward: [(0, '-125.160')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:54,905][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000001013_1037312.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:54,963][20432] Removing /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000613_627712.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:46:58,547][20445] Updated weights for policy 0, policy_version 1020 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:59,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1676.6). Total num frames: 1046528. Throughput: 0: 1745.9. Samples: 1043496. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:46:59,899][12030] Avg episode reward: [(0, '-129.613')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:04,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1655.5, 300 sec: 1673.1). Total num frames: 1051648. Throughput: 0: 1706.8. Samples: 1051863. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:04,905][12030] Avg episode reward: [(0, '-128.692')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:06,918][20445] Updated weights for policy 0, policy_version 1030 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:09,897][12030] Fps is (10 sec: 1228.8, 60 sec: 1655.5, 300 sec: 1676.6). Total num frames: 1058816. Throughput: 0: 1652.3. Samples: 1059809. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:09,899][12030] Avg episode reward: [(0, '-127.795')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:13,206][20445] Updated weights for policy 0, policy_version 1040 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:14,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1655.5, 300 sec: 1680.1). Total num frames: 1067008. Throughput: 0: 1658.5. Samples: 1064580. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:14,899][12030] Avg episode reward: [(0, '-126.673')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:19,333][20445] Updated weights for policy 0, policy_version 1050 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:19,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1655.5, 300 sec: 1680.1). Total num frames: 1075200. Throughput: 0: 1640.6. Samples: 1074413. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:19,899][12030] Avg episode reward: [(0, '-124.353')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:24,834][20445] Updated weights for policy 0, policy_version 1060 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:24,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1687.0). Total num frames: 1085440. Throughput: 0: 1668.5. Samples: 1085654. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:24,899][12030] Avg episode reward: [(0, '-121.635')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:29,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1687.0). Total num frames: 1093632. Throughput: 0: 1663.9. Samples: 1090261. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:29,899][12030] Avg episode reward: [(0, '-103.992')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:30,735][20445] Updated weights for policy 0, policy_version 1070 (0.0005)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:34,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1672.5, 300 sec: 1687.0). Total num frames: 1101824. Throughput: 0: 1648.3. Samples: 1100896. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:34,899][12030] Avg episode reward: [(0, '-104.327')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:37,562][20445] Updated weights for policy 0, policy_version 1080 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:39,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1655.5, 300 sec: 1683.5). Total num frames: 1108992. Throughput: 0: 1583.4. Samples: 1109446. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:39,899][12030] Avg episode reward: [(0, '-101.183')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:43,981][20445] Updated weights for policy 0, policy_version 1090 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:44,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1638.4, 300 sec: 1683.5). Total num frames: 1117184. Throughput: 0: 1575.9. Samples: 1114410. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:44,899][12030] Avg episode reward: [(0, '-90.123')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:49,795][20445] Updated weights for policy 0, policy_version 1100 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:49,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1638.4, 300 sec: 1690.5). Total num frames: 1126400. Throughput: 0: 1629.0. Samples: 1125169. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:49,899][12030] Avg episode reward: [(0, '-89.780')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:54,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1604.3, 300 sec: 1687.0). Total num frames: 1133568. Throughput: 0: 1659.7. Samples: 1134497. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:54,899][12030] Avg episode reward: [(0, '-89.315')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:47:56,300][20445] Updated weights for policy 0, policy_version 1110 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:59,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1604.3, 300 sec: 1697.4). Total num frames: 1142784. Throughput: 0: 1665.6. Samples: 1139532. Policy #0 lag: (min: 0.0, avg: 0.4, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:47:59,900][12030] Avg episode reward: [(0, '-86.352')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:02,296][20445] Updated weights for policy 0, policy_version 1120 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:04,905][12030] Fps is (10 sec: 1432.5, 60 sec: 1604.1, 300 sec: 1690.4). Total num frames: 1147904. Throughput: 0: 1640.7. Samples: 1148256. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:04,910][12030] Avg episode reward: [(0, '-83.890')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1621.3, 300 sec: 1690.5). Total num frames: 1156096. Throughput: 0: 1583.4. Samples: 1156909. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:09,899][12030] Avg episode reward: [(0, '-83.032')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:10,059][20445] Updated weights for policy 0, policy_version 1130 (0.0009)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:14,897][12030] Fps is (10 sec: 1639.7, 60 sec: 1621.3, 300 sec: 1690.5). Total num frames: 1164288. Throughput: 0: 1587.7. Samples: 1161707. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:14,899][12030] Avg episode reward: [(0, '-85.904')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:16,325][20445] Updated weights for policy 0, policy_version 1140 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:19,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1621.3, 300 sec: 1687.0). Total num frames: 1172480. Throughput: 0: 1562.4. Samples: 1171204. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:19,899][12030] Avg episode reward: [(0, '-87.181')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:23,189][20445] Updated weights for policy 0, policy_version 1150 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:24,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1570.1, 300 sec: 1683.5). Total num frames: 1179648. Throughput: 0: 1567.5. Samples: 1179985. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:24,899][12030] Avg episode reward: [(0, '-88.515')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:29,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1553.1, 300 sec: 1676.6). Total num frames: 1186816. Throughput: 0: 1562.3. Samples: 1184712. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:29,899][12030] Avg episode reward: [(0, '-91.329')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:29,959][20445] Updated weights for policy 0, policy_version 1160 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:34,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1587.2, 300 sec: 1680.1). Total num frames: 1197056. Throughput: 0: 1564.4. Samples: 1195567. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:34,899][12030] Avg episode reward: [(0, '-95.952')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:35,412][20445] Updated weights for policy 0, policy_version 1170 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:39,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1587.2, 300 sec: 1676.6). Total num frames: 1204224. Throughput: 0: 1570.1. Samples: 1205151. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:39,899][12030] Avg episode reward: [(0, '-100.547')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:41,962][20445] Updated weights for policy 0, policy_version 1180 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:44,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1604.3, 300 sec: 1676.6). Total num frames: 1213440. Throughput: 0: 1563.0. Samples: 1209865. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:44,898][12030] Avg episode reward: [(0, '-103.193')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:47,933][20445] Updated weights for policy 0, policy_version 1190 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:49,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1587.2, 300 sec: 1680.1). Total num frames: 1221632. Throughput: 0: 1597.1. Samples: 1220113. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:49,899][12030] Avg episode reward: [(0, '-105.853')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:54,256][20445] Updated weights for policy 0, policy_version 1200 (0.0013)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:54,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1587.2, 300 sec: 1676.6). Total num frames: 1228800. Throughput: 0: 1621.4. Samples: 1229870. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:54,899][12030] Avg episode reward: [(0, '-105.660')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:48:54,908][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000001201_1229824.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:54,959][20432] Removing /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000000811_830464.pth\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:59,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1587.2, 300 sec: 1687.0). Total num frames: 1238016. Throughput: 0: 1622.1. Samples: 1234701. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:48:59,898][12030] Avg episode reward: [(0, '-100.757')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:00,350][20445] Updated weights for policy 0, policy_version 1210 (0.0011)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:04,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1604.5, 300 sec: 1683.5). Total num frames: 1244160. Throughput: 0: 1613.5. Samples: 1243812. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:04,899][12030] Avg episode reward: [(0, '-101.292')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:08,526][20445] Updated weights for policy 0, policy_version 1220 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:09,897][12030] Fps is (10 sec: 1228.8, 60 sec: 1570.1, 300 sec: 1673.1). Total num frames: 1250304. Throughput: 0: 1582.5. Samples: 1251196. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:09,899][12030] Avg episode reward: [(0, '-103.743')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:14,777][20445] Updated weights for policy 0, policy_version 1230 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:14,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1587.2, 300 sec: 1680.1). Total num frames: 1259520. Throughput: 0: 1584.4. Samples: 1256009. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:14,899][12030] Avg episode reward: [(0, '-101.842')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:19,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1587.2, 300 sec: 1676.6). Total num frames: 1267712. Throughput: 0: 1588.5. Samples: 1267050. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:19,899][12030] Avg episode reward: [(0, '-100.349')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:20,915][20445] Updated weights for policy 0, policy_version 1240 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:24,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1604.3, 300 sec: 1673.1). Total num frames: 1275904. Throughput: 0: 1583.4. Samples: 1276404. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:24,899][12030] Avg episode reward: [(0, '-93.637')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:26,980][20445] Updated weights for policy 0, policy_version 1250 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:29,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1621.3, 300 sec: 1669.6). Total num frames: 1284096. Throughput: 0: 1591.6. Samples: 1281489. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:29,899][12030] Avg episode reward: [(0, '-92.894')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:34,209][20445] Updated weights for policy 0, policy_version 1260 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:34,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1570.1, 300 sec: 1659.2). Total num frames: 1291264. Throughput: 0: 1551.1. Samples: 1289914. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:34,899][12030] Avg episode reward: [(0, '-91.742')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:39,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1587.2, 300 sec: 1659.2). Total num frames: 1299456. Throughput: 0: 1556.6. Samples: 1299919. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:39,899][12030] Avg episode reward: [(0, '-91.986')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:40,344][20445] Updated weights for policy 0, policy_version 1270 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:44,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1553.1, 300 sec: 1645.3). Total num frames: 1306624. Throughput: 0: 1538.3. Samples: 1303923. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:44,899][12030] Avg episode reward: [(0, '-92.622')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:46,788][20445] Updated weights for policy 0, policy_version 1280 (0.0011)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:49,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1553.1, 300 sec: 1641.9). Total num frames: 1314816. Throughput: 0: 1564.8. Samples: 1314228. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:49,899][12030] Avg episode reward: [(0, '-93.334')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:53,665][20445] Updated weights for policy 0, policy_version 1290 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:54,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1553.1, 300 sec: 1634.9). Total num frames: 1321984. Throughput: 0: 1591.0. Samples: 1322793. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:54,899][12030] Avg episode reward: [(0, '-93.723')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:49:59,553][20445] Updated weights for policy 0, policy_version 1300 (0.0005)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:59,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1553.1, 300 sec: 1641.9). Total num frames: 1331200. Throughput: 0: 1606.4. Samples: 1328295. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:49:59,899][12030] Avg episode reward: [(0, '-97.369')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:04,897][12030] Fps is (10 sec: 1433.6, 60 sec: 1536.0, 300 sec: 1628.0). Total num frames: 1336320. Throughput: 0: 1541.0. Samples: 1336394. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:04,899][12030] Avg episode reward: [(0, '-99.947')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:08,639][20445] Updated weights for policy 0, policy_version 1310 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:09,897][12030] Fps is (10 sec: 1228.8, 60 sec: 1553.1, 300 sec: 1617.6). Total num frames: 1343488. Throughput: 0: 1499.3. Samples: 1343872. Policy #0 lag: (min: 0.0, avg: 0.4, max: 2.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:09,898][12030] Avg episode reward: [(0, '-108.930')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:13,602][20445] Updated weights for policy 0, policy_version 1320 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:14,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1570.1, 300 sec: 1621.0). Total num frames: 1353728. Throughput: 0: 1525.1. Samples: 1350118. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:14,899][12030] Avg episode reward: [(0, '-122.223')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:19,168][20445] Updated weights for policy 0, policy_version 1330 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:19,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1587.2, 300 sec: 1617.6). Total num frames: 1362944. Throughput: 0: 1589.2. Samples: 1361428. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:19,899][12030] Avg episode reward: [(0, '-132.371')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:24,291][20445] Updated weights for policy 0, policy_version 1340 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:24,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1621.3, 300 sec: 1621.0). Total num frames: 1373184. Throughput: 0: 1632.4. Samples: 1373375. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:24,899][12030] Avg episode reward: [(0, '-139.474')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:29,595][20445] Updated weights for policy 0, policy_version 1350 (0.0008)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:29,897][12030] Fps is (10 sec: 1945.6, 60 sec: 1638.4, 300 sec: 1621.0). Total num frames: 1382400. Throughput: 0: 1677.4. Samples: 1379404. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:29,899][12030] Avg episode reward: [(0, '-137.333')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:34,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1628.0). Total num frames: 1391616. Throughput: 0: 1681.8. Samples: 1389911. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:34,899][12030] Avg episode reward: [(0, '-131.627')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:35,426][20445] Updated weights for policy 0, policy_version 1360 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:39,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1689.6, 300 sec: 1628.0). Total num frames: 1400832. Throughput: 0: 1741.1. Samples: 1401143. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:39,899][12030] Avg episode reward: [(0, '-120.557')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:41,027][20445] Updated weights for policy 0, policy_version 1370 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:44,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1706.7, 300 sec: 1628.0). Total num frames: 1409024. Throughput: 0: 1721.9. Samples: 1405780. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:44,899][12030] Avg episode reward: [(0, '-109.344')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:47,164][20445] Updated weights for policy 0, policy_version 1380 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:49,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1706.7, 300 sec: 1628.0). Total num frames: 1417216. Throughput: 0: 1766.8. Samples: 1415898. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:49,899][12030] Avg episode reward: [(0, '-101.455')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:53,564][20445] Updated weights for policy 0, policy_version 1390 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:54,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1706.7, 300 sec: 1621.0). Total num frames: 1424384. Throughput: 0: 1812.0. Samples: 1425411. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:54,899][12030] Avg episode reward: [(0, '-94.922')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:54,905][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000001391_1424384.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:54,957][20432] Removing /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000001013_1037312.pth\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:50:59,708][20445] Updated weights for policy 0, policy_version 1400 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:59,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1706.7, 300 sec: 1631.5). Total num frames: 1433600. Throughput: 0: 1784.5. Samples: 1430419. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:50:59,898][12030] Avg episode reward: [(0, '-83.101')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:04,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1723.7, 300 sec: 1628.0). Total num frames: 1439744. Throughput: 0: 1733.0. Samples: 1439413. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:04,899][12030] Avg episode reward: [(0, '-79.330')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:07,895][20445] Updated weights for policy 0, policy_version 1410 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1723.7, 300 sec: 1624.5). Total num frames: 1446912. Throughput: 0: 1638.6. Samples: 1447114. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:09,899][12030] Avg episode reward: [(0, '-73.532')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:13,934][20445] Updated weights for policy 0, policy_version 1420 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:14,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1689.6, 300 sec: 1624.5). Total num frames: 1455104. Throughput: 0: 1620.7. Samples: 1452337. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:14,899][12030] Avg episode reward: [(0, '-71.031')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:19,543][20445] Updated weights for policy 0, policy_version 1430 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:19,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1689.6, 300 sec: 1624.5). Total num frames: 1464320. Throughput: 0: 1626.8. Samples: 1463119. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:19,898][12030] Avg episode reward: [(0, '-76.478')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:24,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1672.5, 300 sec: 1628.0). Total num frames: 1473536. Throughput: 0: 1616.9. Samples: 1473903. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:24,899][12030] Avg episode reward: [(0, '-82.472')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:25,270][20445] Updated weights for policy 0, policy_version 1440 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:29,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1655.5, 300 sec: 1628.0). Total num frames: 1481728. Throughput: 0: 1630.9. Samples: 1479169. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:29,898][12030] Avg episode reward: [(0, '-98.206')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:31,350][20445] Updated weights for policy 0, policy_version 1450 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:34,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1655.5, 300 sec: 1631.5). Total num frames: 1490944. Throughput: 0: 1633.4. Samples: 1489400. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:34,899][12030] Avg episode reward: [(0, '-105.334')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:37,072][20445] Updated weights for policy 0, policy_version 1460 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:39,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1655.5, 300 sec: 1631.5). Total num frames: 1500160. Throughput: 0: 1668.4. Samples: 1500491. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:39,898][12030] Avg episode reward: [(0, '-118.270')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:42,789][20445] Updated weights for policy 0, policy_version 1470 (0.0013)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:44,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1655.5, 300 sec: 1628.0). Total num frames: 1508352. Throughput: 0: 1669.0. Samples: 1505524. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:44,899][12030] Avg episode reward: [(0, '-119.781')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:49,025][20445] Updated weights for policy 0, policy_version 1480 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:49,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1655.5, 300 sec: 1624.5). Total num frames: 1516544. Throughput: 0: 1687.8. Samples: 1515362. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:49,899][12030] Avg episode reward: [(0, '-122.683')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:54,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1672.5, 300 sec: 1621.0). Total num frames: 1524736. Throughput: 0: 1743.8. Samples: 1525584. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:54,899][12030] Avg episode reward: [(0, '-123.760')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:51:55,028][20445] Updated weights for policy 0, policy_version 1490 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:59,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1655.5, 300 sec: 1631.5). Total num frames: 1532928. Throughput: 0: 1735.1. Samples: 1530416. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:51:59,899][12030] Avg episode reward: [(0, '-119.873')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:01,624][20445] Updated weights for policy 0, policy_version 1500 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:04,899][12030] Fps is (10 sec: 1331.0, 60 sec: 1638.4, 300 sec: 1624.5). Total num frames: 1538048. Throughput: 0: 1669.6. Samples: 1538252. Policy #0 lag: (min: 0.0, avg: 0.2, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:04,905][12030] Avg episode reward: [(0, '-114.205')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:09,718][20445] Updated weights for policy 0, policy_version 1510 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:09,897][12030] Fps is (10 sec: 1331.2, 60 sec: 1655.5, 300 sec: 1624.5). Total num frames: 1546240. Throughput: 0: 1616.9. Samples: 1546664. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:09,898][12030] Avg episode reward: [(0, '-113.492')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:14,897][12030] Fps is (10 sec: 1536.3, 60 sec: 1638.4, 300 sec: 1621.0). Total num frames: 1553408. Throughput: 0: 1602.1. Samples: 1551265. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:14,899][12030] Avg episode reward: [(0, '-109.607')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:16,041][20445] Updated weights for policy 0, policy_version 1520 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:19,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1638.4, 300 sec: 1617.6). Total num frames: 1562624. Throughput: 0: 1609.0. Samples: 1561806. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:19,899][12030] Avg episode reward: [(0, '-110.201')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:21,690][20445] Updated weights for policy 0, policy_version 1530 (0.0007)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:24,897][12030] Fps is (10 sec: 1843.2, 60 sec: 1638.4, 300 sec: 1621.0). Total num frames: 1571840. Throughput: 0: 1592.8. Samples: 1572169. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:24,898][12030] Avg episode reward: [(0, '-117.946')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:27,454][20445] Updated weights for policy 0, policy_version 1540 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:29,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1638.4, 300 sec: 1621.0). Total num frames: 1580032. Throughput: 0: 1605.6. Samples: 1577778. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:29,899][12030] Avg episode reward: [(0, '-120.198')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:34,042][20445] Updated weights for policy 0, policy_version 1550 (0.0006)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:34,897][12030] Fps is (10 sec: 1638.4, 60 sec: 1621.3, 300 sec: 1624.5). Total num frames: 1588224. Throughput: 0: 1589.9. Samples: 1586908. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:34,899][12030] Avg episode reward: [(0, '-119.344')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:39,787][20445] Updated weights for policy 0, policy_version 1560 (0.0010)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:39,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1621.3, 300 sec: 1628.0). Total num frames: 1597440. Throughput: 0: 1601.5. Samples: 1597653. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:39,898][12030] Avg episode reward: [(0, '-118.476')]\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:44,897][12030] Fps is (10 sec: 1740.8, 60 sec: 1621.3, 300 sec: 1624.5). Total num frames: 1605632. Throughput: 0: 1617.3. Samples: 1603193. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:44,899][12030] Avg episode reward: [(0, '-115.117')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:45,607][20445] Updated weights for policy 0, policy_version 1570 (0.0005)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,897][12030] Fps is (10 sec: 1536.0, 60 sec: 1604.3, 300 sec: 1624.5). Total num frames: 1612800. Throughput: 0: 1646.4. Samples: 1612339. Policy #0 lag: (min: 0.0, avg: 0.3, max: 1.0)\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,899][12030] Avg episode reward: [(0, '-114.694')]\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:49,945][12030] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 12030], exiting...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,946][20432] Stopping Batcher_0...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:49,947][12030] Runner profile tree view:\n",
      "main_loop: 953.8514\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,947][20432] Loop batcher_evt_loop terminating...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:49,947][20432] Saving /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander_example/checkpoint_p0/checkpoint_000001576_1613824.pth...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,946][20447] Stopping RolloutWorker_w3...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-26 22:52:49,948][12030] Collected {0: 1613824}, FPS: 1691.9\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,947][20447] Loop rollout_proc3_evt_loop terminating...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[2022-10-26 22:52:49,952][20457] Stopping RolloutWorker_w4...\u001b[0m\n",
      "\u001b[36m[2022-10-26 22:52:49,953][20459] Stopping RolloutWorker_w5...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sample_factory.train import run_rl\n",
    "\n",
    "run_rl(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Evaluating the Model and Uploading to the Hugging Face Hub**\n",
    "\n",
    "After training the model, we can use the `enjoy` function to see how well it did. `enjoy` also allows us to generate a video with the `--save_video` flag, as well as upload the model to the Hub using the `--push_to_hub` flag. Make sure you also specify `--hf_repository` with your Hugging Face username and repository name in the form `<username>/<repo_name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2022-10-27 19:50:18,805][14511] Loading existing experiment configuration from /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander2/cfg.json\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,807][14511] Adding new argument 'fps'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,808][14511] Adding new argument 'eval_env_frameskip'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,808][14511] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,809][14511] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,809][14511] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,810][14511] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,810][14511] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,811][14511] Adding new argument 'max_num_episodes'=5 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,812][14511] Adding new argument 'push_to_hub'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,812][14511] Adding new argument 'hf_repository'='andrewzhang505/sf2-lunar-lander' that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,812][14511] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,813][14511] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 19:50:18,814][14511] Using frameskip 1 and render_action_repeat=1 for evaluation\u001b[0m\n",
      "\u001b[33m[2022-10-27 19:50:22,032][14511] Loading state from checkpoint /home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander2/checkpoint_p0/checkpoint_000002708_2772992.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:28,649][14511] Avg episode rewards: #0: 121.328, true rewards: #0: 121.328\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:28,650][14511] Avg episode reward: 121.328, avg true_objective: 121.328\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:30,543][14511] Avg episode rewards: #0: -7.019, true rewards: #0: -7.019\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:30,544][14511] Avg episode reward: -7.019, avg true_objective: -7.019\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:33,722][14511] Avg episode rewards: #0: 56.009, true rewards: #0: 56.009\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:33,723][14511] Avg episode reward: 56.009, avg true_objective: 56.009\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:36,038][14511] Avg episode rewards: #0: 97.523, true rewards: #0: 97.523\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:36,039][14511] Avg episode reward: 97.523, avg true_objective: 97.523\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:38,637][14511] Avg episode rewards: #0: 126.577, true rewards: #0: 126.577\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:50:38,638][14511] Avg episode reward: 126.577, avg true_objective: 126.577\u001b[0m\n",
      "ffmpeg version 5.0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 10.3.0 (conda-forge gcc 10.3.0-16)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-pthreads --enable-vaapi --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/pkg-config\n",
      "  libavutil      57. 17.100 / 57. 17.100\n",
      "  libavcodec     59. 18.100 / 59. 18.100\n",
      "  libavformat    59. 16.100 / 59. 16.100\n",
      "  libavdevice    59.  4.100 / 59.  4.100\n",
      "  libavfilter     8. 24.100 /  8. 24.100\n",
      "  libswscale      6.  4.100 /  6.  4.100\n",
      "  libswresample   4.  3.100 /  4.  3.100\n",
      "  libpostproc    56.  3.100 / 56.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_andrew_huggingface_co/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:01:26.53, start: 0.000000, bitrate: 325 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 600x400 [SAR 1:1 DAR 3:2], 323 kb/s, 30 fps, 30 tbr, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x558e5b35eec0] using SAR=1/1\n",
      "[libx264 @ 0x558e5b35eec0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x558e5b35eec0] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x558e5b35eec0] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/andrew_huggingface_co/sample-factory/train_dir/lunar_lander2/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.16.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(progressive), 600x400 [SAR 1:1 DAR 3:2], q=2-31, 30 fps, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc59.18.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame= 2596 fps=950 q=-1.0 Lsize=     512kB time=00:01:26.43 bitrate=  48.6kbits/s speed=31.6x    \n",
      "video:481kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 6.425062%\n",
      "[libx264 @ 0x558e5b35eec0] frame I:11    Avg QP: 7.55  size:  2527\n",
      "[libx264 @ 0x558e5b35eec0] frame P:691   Avg QP:17.74  size:   277\n",
      "[libx264 @ 0x558e5b35eec0] frame B:1894  Avg QP:21.56  size:   144\n",
      "[libx264 @ 0x558e5b35eec0] consecutive B-frames:  1.2%  3.0%  5.1% 90.8%\n",
      "[libx264 @ 0x558e5b35eec0] mb I  I16..4: 91.0%  2.9%  6.1%\n",
      "[libx264 @ 0x558e5b35eec0] mb P  I16..4:  0.2%  0.4%  0.2%  P16..4:  1.4%  0.5%  0.2%  0.0%  0.0%    skip:97.1%\n",
      "[libx264 @ 0x558e5b35eec0] mb B  I16..4:  0.1%  0.1%  0.1%  B16..8:  2.4%  0.3%  0.1%  direct: 0.1%  skip:96.9%  L0:55.7% L1:42.6% BI: 1.6%\n",
      "[libx264 @ 0x558e5b35eec0] 8x8 transform intra:23.8% inter:19.8%\n",
      "[libx264 @ 0x558e5b35eec0] coded y,uvDC,uvAC intra: 8.9% 18.3% 15.1% inter: 0.2% 0.4% 0.3%\n",
      "[libx264 @ 0x558e5b35eec0] i16 v,h,dc,p: 86%  8%  5%  0%\n",
      "[libx264 @ 0x558e5b35eec0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  5% 10% 84%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x558e5b35eec0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 13% 60%  2%  3%  2%  2%  1%  2%\n",
      "[libx264 @ 0x558e5b35eec0] i8c dc,h,v,p: 83% 11%  6%  0%\n",
      "[libx264 @ 0x558e5b35eec0] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x558e5b35eec0] ref P L0: 60.8%  2.2% 22.5% 14.5%\n",
      "[libx264 @ 0x558e5b35eec0] ref B L0: 65.9% 29.0%  5.2%\n",
      "[libx264 @ 0x558e5b35eec0] ref B L1: 90.8%  9.2%\n",
      "[libx264 @ 0x558e5b35eec0] kb/s:45.50\n",
      "\u001b[37m\u001b[1m[2022-10-27 19:51:07,726][14511] The model has been pushed to https://huggingface.co/andrewzhang505/sf2-lunar-lander\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 126.57706604003906)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sample_factory.enjoy import enjoy\n",
    "\n",
    "## CHANGE THIS\n",
    "username = \"andrewzhang505\"\n",
    "\n",
    "enjoy_args = [\"--no_render\", \"--max_num_episodes=5\", \"--push_to_hub\", f\"--hf_repository={username}/{experiment_name}\", \"--save_video\"]\n",
    "cfg = parse_custom_args(argv=argv+enjoy_args, evaluation=True)\n",
    "enjoy(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"./train_dir/lunar_lander2/replay.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(f\"./train_dir/{experiment_name}/replay.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Downloading Models from the Hub**\n",
    "\n",
    "You can also download other models from the Hub for your own use as well. The following will download the model to `./train_dir/sf2-lunar-lander/` and you can use the model by specifying `--experiment=sf2-lunar-lander`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_huggingface_co/sample-factory/./train_dir/sf2-lunar-lander is already a clone of https://huggingface.co/andrewzhang505/sf2-lunar-lander. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "\u001b[37m\u001b[1m[2022-10-27 21:07:10,839][31193] The repository andrewzhang505/sf2-lunar-lander has been cloned to ./train_dir/sf2-lunar-lander\u001b[0m\n",
      "\u001b[33m[2022-10-27 21:07:10,862][31193] Loading existing experiment configuration from /home/andrew_huggingface_co/sample-factory/train_dir/sf2-lunar-lander/cfg.json\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,864][31193] Overriding arg 'experiment' with value 'sf2-lunar-lander' passed from command line\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,864][31193] Adding new argument 'fps'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,865][31193] Adding new argument 'eval_env_frameskip'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,865][31193] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,865][31193] Adding new argument 'save_video'=True that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,866][31193] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,866][31193] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,867][31193] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,867][31193] Adding new argument 'max_num_episodes'=1 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,868][31193] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,868][31193] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,869][31193] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,869][31193] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
      "\u001b[36m[2022-10-27 21:07:10,870][31193] Using frameskip 1 and render_action_repeat=1 for evaluation\u001b[0m\n",
      "\u001b[33m[2022-10-27 21:07:14,128][31193] Loading state from checkpoint /home/andrew_huggingface_co/sample-factory/train_dir/sf2-lunar-lander/checkpoint_p0/checkpoint_000002708_2772992.pth...\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 21:07:16,663][31193] Avg episode rewards: #0: 190.743, true rewards: #0: 190.743\u001b[0m\n",
      "\u001b[37m\u001b[1m[2022-10-27 21:07:16,664][31193] Avg episode reward: 190.743, avg true_objective: 190.743\u001b[0m\n",
      "ffmpeg version 5.0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 10.3.0 (conda-forge gcc 10.3.0-16)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-pthreads --enable-vaapi --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1657987167490/_build_env/bin/pkg-config\n",
      "  libavutil      57. 17.100 / 57. 17.100\n",
      "  libavcodec     59. 18.100 / 59. 18.100\n",
      "  libavformat    59. 16.100 / 59. 16.100\n",
      "  libavdevice    59.  4.100 / 59.  4.100\n",
      "  libavfilter     8. 24.100 /  8. 24.100\n",
      "  libswscale      6.  4.100 /  6.  4.100\n",
      "  libswresample   4.  3.100 /  4.  3.100\n",
      "  libpostproc    56.  3.100 / 56.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/sf2_andrew_huggingface_co/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Duration: 00:00:11.67, start: 0.000000, bitrate: 337 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 600x400 [SAR 1:1 DAR 3:2], 335 kb/s, 30 fps, 30 tbr, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x5625376c92c0] using SAR=1/1\n",
      "[libx264 @ 0x5625376c92c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x5625376c92c0] profile High, level 3.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x5625376c92c0] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/home/andrew_huggingface_co/sample-factory/train_dir/sf2-lunar-lander/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.16.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(progressive), 600x400 [SAR 1:1 DAR 3:2], q=2-31, 30 fps, 15360 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc59.18.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame=  350 fps=0.0 q=-1.0 Lsize=      80kB time=00:00:11.56 bitrate=  56.5kbits/s speed=26.7x    \n",
      "video:75kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 6.512665%\n",
      "[libx264 @ 0x5625376c92c0] frame I:2     Avg QP: 7.67  size:  2310\n",
      "[libx264 @ 0x5625376c92c0] frame P:97    Avg QP:18.19  size:   314\n",
      "[libx264 @ 0x5625376c92c0] frame B:251   Avg QP:22.19  size:   163\n",
      "[libx264 @ 0x5625376c92c0] consecutive B-frames:  1.4%  5.7%  9.4% 83.4%\n",
      "[libx264 @ 0x5625376c92c0] mb I  I16..4: 88.5%  6.3%  5.3%\n",
      "[libx264 @ 0x5625376c92c0] mb P  I16..4:  0.4%  0.6%  0.3%  P16..4:  1.7%  0.5%  0.2%  0.0%  0.0%    skip:96.3%\n",
      "[libx264 @ 0x5625376c92c0] mb B  I16..4:  0.1%  0.1%  0.1%  B16..8:  3.1%  0.4%  0.1%  direct: 0.1%  skip:96.1%  L0:57.9% L1:40.8% BI: 1.3%\n",
      "[libx264 @ 0x5625376c92c0] 8x8 transform intra:26.3% inter:19.9%\n",
      "[libx264 @ 0x5625376c92c0] coded y,uvDC,uvAC intra: 7.9% 14.9% 12.7% inter: 0.3% 0.4% 0.3%\n",
      "[libx264 @ 0x5625376c92c0] i16 v,h,dc,p: 81% 14%  5%  0%\n",
      "[libx264 @ 0x5625376c92c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  5% 13% 81%  0%  0%  0%  0%  0%  1%\n",
      "[libx264 @ 0x5625376c92c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 14% 15% 60%  2%  3%  1%  2%  1%  2%\n",
      "[libx264 @ 0x5625376c92c0] i8c dc,h,v,p: 83% 11%  5%  0%\n",
      "[libx264 @ 0x5625376c92c0] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x5625376c92c0] ref P L0: 64.6%  2.2% 20.9% 12.3%\n",
      "[libx264 @ 0x5625376c92c0] ref B L0: 63.5% 32.2%  4.2%\n",
      "[libx264 @ 0x5625376c92c0] ref B L1: 91.7%  8.3%\n",
      "[libx264 @ 0x5625376c92c0] kb/s:52.10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"./train_dir/sf2-lunar-lander/replay.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sample_factory.huggingface.huggingface_utils import load_from_hf\n",
    "\n",
    "load_from_hf(\"./train_dir\", \"andrewzhang505/sf2-lunar-lander\")\n",
    "\n",
    "download_args = [\"--algo=APPO\", \"--env=LunarLanderContinuous-v2\", \"--experiment=sf2-lunar-lander\", \"--no_render\", \"--max_num_episodes=1\", \"--save_video\"]\n",
    "cfg = parse_custom_args(argv=download_args, evaluation=True)\n",
    "enjoy(cfg)\n",
    "\n",
    "Video(f\"./train_dir/sf2-lunar-lander/replay.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional Resources**\n",
    "\n",
    "For more information on using Sample-Factory, check out our website at https://alex-petrenko.github.io/sample-factory/ and our github at https://github.com/alex-petrenko/sample-factory/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd5eb279d10053843772e5b13836afa1489cd82e3492b360393a155ea89d875e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
